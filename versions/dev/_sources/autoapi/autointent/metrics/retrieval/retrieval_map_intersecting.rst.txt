autointent.metrics.retrieval.retrieval_map_intersecting
=======================================================

.. py:function:: autointent.metrics.retrieval.retrieval_map_intersecting(query_labels, candidates_labels, k = None)

   Calculate the mean average precision at position k for the intersecting labels.

   The Mean Average Precision (MAP) for intersecting labels is computed as
   the average of the average precision (AP) scores for all queries. The average
   precision for a single query considers the intersecting true and predicted labels for the
   top-k retrieved items.

   MAP is given by:

   .. math::

       \text{MAP} = \frac{1}{Q} \sum_{q=1}^{Q} \text{AP}_{\text{intersecting}}(q, c, k)

   where:
   - :math:`Q` is the total number of queries,
   - :math:`\text{AP}_{\text{intersecting}}(q, c, k)` is the average precision for the
   :math:`q`-th query, calculated using the intersecting true labels (`q`),
   predicted labels (`c`), and the number of top items (`k`) to consider.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model
    (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric

