{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hydra configs for nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra.utils import instantiate\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autointent import Context\n",
    "from autointent.pipeline.optimization import get_db_dir, get_run_name, load_data\n",
    "\n",
    "run_name = get_run_name(\"multiclass-cpu\")\n",
    "db_dir = get_db_dir(\"\", run_name)\n",
    "\n",
    "data = load_data(\"/home/voorhs/repos/AutoIntent/tests/minimal-optimization/data/clinc_subset.json\", multilabel=False)\n",
    "context = Context(\n",
    "    multiclass_intent_records=data,\n",
    "    multilabel_utterance_records=[],\n",
    "    test_utterance_records=[],\n",
    "    device=\"cpu\",\n",
    "    mode=\"multiclass\",\n",
    "    multilabel_generation_config=\"\",\n",
    "    db_dir=db_dir,\n",
    "    regex_sampling=0,\n",
    "    seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autointent.pipeline.optimization import load_config\n",
    "\n",
    "config = load_config(\n",
    "    config_path=\"/home/voorhs/repos/AutoIntent/tests/minimal-optimization/configs/multiclass.yaml\",\n",
    "    multilabel=False,\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nodes': [{'metric': 'retrieval_hit_rate',\n",
      "            'modules': [{'k': [10],\n",
      "                         'model_name': ['sentence-transformers/all-MiniLM-L6-v2',\n",
      "                                        'avsolatorio/GIST-small-Embedding-v0'],\n",
      "                         'module_type': 'vector_db'}],\n",
      "            'node_type': 'retrieval'},\n",
      "           {'metric': 'scoring_roc_auc',\n",
      "            'modules': [{'k': [5, 10],\n",
      "                         'module_type': 'knn',\n",
      "                         'weights': ['uniform', 'distance', 'closest']},\n",
      "                        {'module_type': 'linear'},\n",
      "                        {'k': [1, 3],\n",
      "                         'model_name': ['cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
      "                                        'avsolatorio/GIST-small-Embedding-v0'],\n",
      "                         'module_type': 'dnnc',\n",
      "                         'train_head': [False, True]}],\n",
      "            'node_type': 'scoring'},\n",
      "           {'metric': 'prediction_accuracy',\n",
      "            'modules': [{'module_type': 'threshold',\n",
      "                         'thresh': [0.5, [0.5, 0.5, 0.5]]},\n",
      "                        {'module_type': 'tunable'},\n",
      "                        {'module_type': 'argmax'},\n",
      "                        {'module_type': 'jinoos'}],\n",
      "            'node_type': 'prediction'}]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autointent.configs.modules import MODULES_CONFIGS, create_search_space_dataclass\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def parse_search_space(node_type: str, search_space: dict[str, Any]):\n",
    "    module_config = MODULES_CONFIGS[node_type][search_space[\"module_type\"]]\n",
    "    make_search_space_model = create_search_space_dataclass(module_config)\n",
    "    return make_search_space_model(**search_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type = \"retrieval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autointent.configs.node import NodeOptimizerConfig\n",
    "\n",
    "retrieval_optimizer_config = NodeOptimizerConfig(\n",
    "    node_type=node_type,\n",
    "    search_space=[parse_search_space(node_type, ss) for ss in config[\"nodes\"][0][\"modules\"]],\n",
    "    metric=config[\"nodes\"][0][\"metric\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autointent.nodes.optimization import NodeOptimizer\n",
    "\n",
    "retrieval_optimizer: NodeOptimizer = instantiate(retrieval_optimizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/voorhs/.cache/pypoetry/virtualenvs/autointent-D7M6VOhJ-py3.12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "retrieval_optimizer.fit(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type = \"scoring\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_optimizer_config = NodeOptimizerConfig(\n",
    "    node_type=node_type,\n",
    "    search_space=[parse_search_space(node_type, ss) for ss in config[\"nodes\"][1][\"modules\"]],\n",
    "    metric=config[\"nodes\"][1][\"metric\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_optimizer: NodeOptimizer = instantiate(scoring_optimizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avsolatorio/GIST-small-Embedding-v0 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avsolatorio/GIST-small-Embedding-v0 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avsolatorio/GIST-small-Embedding-v0 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avsolatorio/GIST-small-Embedding-v0 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "scoring_optimizer.fit(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type = \"prediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_optimizer_config = NodeOptimizerConfig(\n",
    "    node_type=node_type,\n",
    "    search_space=[parse_search_space(node_type, ss) for ss in config[\"nodes\"][2][\"modules\"]],\n",
    "    metric=config[\"nodes\"][2][\"metric\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_optimizer: NodeOptimizer = instantiate(prediction_optimizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your data contains out-of-scope utterances, but ArgmaxPredictor cannot detect them. Consider different predictor\n"
     ]
    }
   ],
   "source": [
    "prediction_optimizer.fit(context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autointent-D7M6VOhJ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
