{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hydra configs for modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra.utils import instantiate\n",
    "from dataclasses import asdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autointent import Context\n",
    "from autointent.pipeline.optimization import get_db_dir, get_run_name, load_data\n",
    "\n",
    "run_name = get_run_name(\"multiclass-cpu\")\n",
    "db_dir = get_db_dir(\"\", run_name)\n",
    "\n",
    "data = load_data(\"/home/voorhs/repos/AutoIntent/tests/minimal-optimization/data/clinc_subset.json\", multilabel=False)\n",
    "context = Context(\n",
    "    multiclass_intent_records=data,\n",
    "    multilabel_utterance_records=[],\n",
    "    test_utterance_records=[],\n",
    "    device=\"cpu\",\n",
    "    mode=\"multiclass\",\n",
    "    multilabel_generation_config=\"\",\n",
    "    db_dir=db_dir,\n",
    "    regex_sampling=0,\n",
    "    seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autointent.modules import VectorDBModule\n",
    "from autointent.configs.modules import VectorDBConfig\n",
    "from autointent.metrics import retrieval_hit_rate\n",
    "\n",
    "retriever_config = VectorDBConfig(k=3, model_name=\"sergeyzh/rubert-tiny-turbo\")\n",
    "retriever: VectorDBModule = instantiate(retriever_config)\n",
    "\n",
    "retriever.fit(context)\n",
    "metric_value = retriever.score(context, retrieval_hit_rate)\n",
    "artifact = retriever.get_assets()\n",
    "\n",
    "context.optimization_info.log_module_optimization(\n",
    "    node_type=\"retrieval\",\n",
    "    module_type=\"vector_db\",\n",
    "    module_params=asdict(retriever_config),\n",
    "    metric_value=metric_value,\n",
    "    metric_name=\"retrieval_hit_rate\",\n",
    "    artifact=artifact,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autointent.modules import KNNScorer\n",
    "from autointent.configs.modules import KNNScorerConfig\n",
    "# from autointent.configs.modules.scoring.knn import KNNWeightsType\n",
    "from autointent.metrics import scoring_roc_auc\n",
    "\n",
    "scorer_config = KNNScorerConfig(k=3, weights=\"distance\")\n",
    "scorer: KNNScorer = instantiate(scorer_config)\n",
    "\n",
    "scorer.fit(context)\n",
    "metric_value = scorer.score(context, scoring_roc_auc)\n",
    "artifact = scorer.get_assets()\n",
    "\n",
    "context.optimization_info.log_module_optimization(\n",
    "    node_type=\"scoring\",\n",
    "    module_type=\"knn\",\n",
    "    module_params=asdict(scorer_config),\n",
    "    metric_value=metric_value,\n",
    "    metric_name=\"scoring_roc_auc\",\n",
    "    artifact=artifact,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from autointent.modules import LinearScorer\n",
    "from autointent.configs.modules import LinearScorerConfig\n",
    "from autointent.metrics import scoring_roc_auc\n",
    "\n",
    "scorer_config = LinearScorerConfig()\n",
    "scorer: LinearScorer = instantiate(scorer_config)\n",
    "\n",
    "scorer.fit(context)\n",
    "metric_value = scorer.score(context, scoring_roc_auc)\n",
    "artifact = scorer.get_assets()\n",
    "\n",
    "context.optimization_info.log_module_optimization(\n",
    "    node_type=\"scoring\",\n",
    "    module_type=\"linear\",\n",
    "    module_params=asdict(scorer_config),\n",
    "    metric_value=metric_value,\n",
    "    metric_name=\"scoring_roc_auc\",\n",
    "    artifact=artifact,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autointent.modules import DNNCScorer\n",
    "from autointent.configs.modules import DNNCScorerConfig\n",
    "from autointent.metrics import scoring_roc_auc\n",
    "\n",
    "scorer_config = DNNCScorerConfig(model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", k=3, train_head=True)\n",
    "scorer: DNNCScorer = instantiate(scorer_config)\n",
    "\n",
    "scorer.fit(context)\n",
    "metric_value = scorer.score(context, scoring_roc_auc)\n",
    "artifact = scorer.get_assets()\n",
    "\n",
    "context.optimization_info.log_module_optimization(\n",
    "    node_type=\"scoring\",\n",
    "    module_type=\"dnnc\",\n",
    "    module_params=asdict(scorer_config),\n",
    "    metric_value=metric_value,\n",
    "    metric_name=\"scoring_roc_auc\",\n",
    "    artifact=artifact,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autointent.modules import ThresholdPredictor\n",
    "from autointent.configs.modules import ThresholdPredictorConfig\n",
    "from autointent.metrics import prediction_accuracy\n",
    "\n",
    "predictor_config = ThresholdPredictorConfig(thresh=0.5)\n",
    "predictor: ThresholdPredictor = instantiate(predictor_config)\n",
    "\n",
    "predictor.fit(context)\n",
    "metric_value = predictor.score(context, prediction_accuracy)\n",
    "artifact = predictor.get_assets()\n",
    "\n",
    "context.optimization_info.log_module_optimization(\n",
    "    node_type=\"prediction\",\n",
    "    module_type=\"threshold\",\n",
    "    module_params=asdict(predictor_config),\n",
    "    metric_value=metric_value,\n",
    "    metric_name=\"prediction_accuracy\",\n",
    "    artifact=artifact,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-07 11:29:23,904] A new study created in memory with name: no-name-6ae90f0e-5f5a-4484-a6d2-bec67e7f1cad\n"
     ]
    }
   ],
   "source": [
    "from autointent.modules import TunablePredictor\n",
    "from autointent.configs.modules import TunablePredictorConfig\n",
    "from autointent.metrics import prediction_accuracy\n",
    "\n",
    "predictor_config = TunablePredictorConfig(n_trials=100)\n",
    "predictor: TunablePredictor = instantiate(predictor_config)\n",
    "\n",
    "predictor.fit(context)\n",
    "metric_value = predictor.score(context, prediction_accuracy)\n",
    "artifact = predictor.get_assets()\n",
    "\n",
    "context.optimization_info.log_module_optimization(\n",
    "    node_type=\"prediction\",\n",
    "    module_type=\"tunable\",\n",
    "    module_params=asdict(predictor_config),\n",
    "    metric_value=metric_value,\n",
    "    metric_name=\"prediction_accuracy\",\n",
    "    artifact=artifact,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your data contains out-of-scope utterances, but ArgmaxPredictor cannot detect them. Consider different predictor\n"
     ]
    }
   ],
   "source": [
    "from autointent.modules import ArgmaxPredictor\n",
    "from autointent.configs.modules import ArgmaxPredictorConfig\n",
    "from autointent.metrics import prediction_accuracy\n",
    "\n",
    "predictor_config = ArgmaxPredictorConfig()\n",
    "predictor: ArgmaxPredictor = instantiate(predictor_config)\n",
    "\n",
    "predictor.fit(context)\n",
    "metric_value = predictor.score(context, prediction_accuracy)\n",
    "artifact = predictor.get_assets()\n",
    "\n",
    "context.optimization_info.log_module_optimization(\n",
    "    node_type=\"prediction\",\n",
    "    module_type=\"argmax\",\n",
    "    module_params=asdict(predictor_config),\n",
    "    metric_value=metric_value,\n",
    "    metric_name=\"prediction_accuracy\",\n",
    "    artifact=artifact,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autointent.modules import JinoosPredictor\n",
    "from autointent.configs.modules import JinoosPredictorConfig\n",
    "from autointent.metrics import prediction_accuracy\n",
    "\n",
    "predictor_config = JinoosPredictorConfig()\n",
    "predictor: JinoosPredictor = instantiate(predictor_config)\n",
    "\n",
    "predictor.fit(context)\n",
    "metric_value = predictor.score(context, prediction_accuracy)\n",
    "artifact = predictor.get_assets()\n",
    "\n",
    "context.optimization_info.log_module_optimization(\n",
    "    node_type=\"prediction\",\n",
    "    module_type=\"jinoos\",\n",
    "    module_params=asdict(predictor_config),\n",
    "    metric_value=metric_value,\n",
    "    metric_name=\"prediction_accuracy\",\n",
    "    artifact=artifact,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autointent-D7M6VOhJ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
