{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autointent.nodes.optimization import NodeOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autointent import Context\n",
    "from autointent.pipeline.optimization.utils import get_run_name, load_data, get_db_dir\n",
    "\n",
    "\n",
    "run_name = get_run_name(\"multiclass-cpu\")\n",
    "db_dir = get_db_dir(\"\", run_name)\n",
    "\n",
    "data = load_data(\"/home/voorhs/repos/AutoIntent/tests/minimal_optimization/data/clinc_subset.json\", multilabel=False)\n",
    "context = Context(\n",
    "    multiclass_intent_records=data,\n",
    "    multilabel_utterance_records=[],\n",
    "    test_utterance_records=[],\n",
    "    device=\"cpu\",\n",
    "    mode=\"multiclass\",\n",
    "    multilabel_generation_config=\"\",\n",
    "    db_dir=db_dir,\n",
    "    regex_sampling=0,\n",
    "    seed=0,\n",
    "    dump_dir=\"modules_dumps_multiclass\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_optimizer_config = {\n",
    "    'metric': 'retrieval_hit_rate_intersecting',\n",
    "    'node_type': 'retrieval',\n",
    "    'search_space': [\n",
    "        {\n",
    "            'k': [10],\n",
    "            'model_name': ['deepvk/USER-bge-m3'],\n",
    "            'module_type': 'vector_db'\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_optimizer = NodeOptimizer.from_dict_config(retrieval_optimizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_optimizer.fit(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_optimizer_config = {\n",
    "    'metric': 'scoring_roc_auc',\n",
    "    'node_type': 'scoring',\n",
    "    'search_space': [\n",
    "        {\n",
    "            'k': [3],\n",
    "            'module_type': 'knn',\n",
    "            'weights': ['uniform', 'distance', 'closest']\n",
    "        },\n",
    "        {\n",
    "            'module_type': 'linear'\n",
    "        },\n",
    "        # {\n",
    "        #     \"module_type\": \"mlknn\",\n",
    "        #     \"k\": [5]\n",
    "        # },\n",
    "        {\n",
    "            \"module_type\": \"dnnc\",\n",
    "            \"model_name\": [\"cross-encoder/ms-marco-MiniLM-L-6-v2\"],\n",
    "            \"k\": [3],\n",
    "            \"train_head\": [False, True]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_optimizer = NodeOptimizer.from_dict_config(scoring_optimizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "scoring_optimizer.fit(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_optimizer_config = {\n",
    "    'metric': 'prediction_accuracy',\n",
    "    'node_type': 'prediction',\n",
    "    'search_space': [\n",
    "        {\n",
    "            'module_type': 'threshold',\n",
    "            'thresh': [0.5]\n",
    "        },\n",
    "        {\n",
    "            'module_type': 'tunable',\n",
    "            'n_trials': [None, 3]\n",
    "        },\n",
    "        {\n",
    "            'module_type': 'argmax',\n",
    "        },\n",
    "        {\n",
    "            'module_type': 'jinoos',\n",
    "        },\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_optimizer = NodeOptimizer.from_dict_config(prediction_optimizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-15 21:31:08,782] A new study created in memory with name: no-name-75e8efe0-2c53-434e-a110-e2d77bdac59a\n",
      "Your data contains out-of-scope utterances, but ArgmaxPredictor cannot detect them. Consider different predictor\n"
     ]
    }
   ],
   "source": [
    "prediction_optimizer.fit(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metrics': {'regexp': [],\n",
       "  'retrieval': [0.5],\n",
       "  'scoring': [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0],\n",
       "  'prediction': [0.5714285714285714,\n",
       "   0.7142857142857143,\n",
       "   0.7142857142857143,\n",
       "   0.5714285714285714,\n",
       "   0.7142857142857143]},\n",
       " 'configs': {'regexp': [],\n",
       "  'retrieval': [{'module_type': 'vector_db',\n",
       "    'module_params': {'k': 10, 'model_name': 'deepvk/USER-bge-m3'},\n",
       "    'metric_name': 'retrieval_hit_rate_intersecting',\n",
       "    'metric_value': 0.5,\n",
       "    'module_dump_dir': 'modules_dumps_multiclass/retrieval/vector_db/comb_0'}],\n",
       "  'scoring': [{'module_type': 'knn',\n",
       "    'module_params': {'k': 3, 'weights': 'uniform'},\n",
       "    'metric_name': 'scoring_roc_auc',\n",
       "    'metric_value': 1.0,\n",
       "    'module_dump_dir': 'modules_dumps_multiclass/scoring/knn/comb_0'},\n",
       "   {'module_type': 'knn',\n",
       "    'module_params': {'k': 3, 'weights': 'distance'},\n",
       "    'metric_name': 'scoring_roc_auc',\n",
       "    'metric_value': 1.0,\n",
       "    'module_dump_dir': 'modules_dumps_multiclass/scoring/knn/comb_1'},\n",
       "   {'module_type': 'knn',\n",
       "    'module_params': {'k': 3, 'weights': 'closest'},\n",
       "    'metric_name': 'scoring_roc_auc',\n",
       "    'metric_value': 1.0,\n",
       "    'module_dump_dir': 'modules_dumps_multiclass/scoring/knn/comb_2'},\n",
       "   {'module_type': 'linear',\n",
       "    'module_params': {},\n",
       "    'metric_name': 'scoring_roc_auc',\n",
       "    'metric_value': 1.0,\n",
       "    'module_dump_dir': 'modules_dumps_multiclass/scoring/linear/comb_0'},\n",
       "   {'module_type': 'dnnc',\n",
       "    'module_params': {'model_name': 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
       "     'k': 3,\n",
       "     'train_head': False},\n",
       "    'metric_name': 'scoring_roc_auc',\n",
       "    'metric_value': 0.6666666666666666,\n",
       "    'module_dump_dir': 'modules_dumps_multiclass/scoring/dnnc/comb_0'},\n",
       "   {'module_type': 'dnnc',\n",
       "    'module_params': {'model_name': 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
       "     'k': 3,\n",
       "     'train_head': True},\n",
       "    'metric_name': 'scoring_roc_auc',\n",
       "    'metric_value': 1.0,\n",
       "    'module_dump_dir': 'modules_dumps_multiclass/scoring/dnnc/comb_1'}],\n",
       "  'prediction': [{'module_type': 'threshold',\n",
       "    'module_params': {'thresh': 0.5},\n",
       "    'metric_name': 'prediction_accuracy',\n",
       "    'metric_value': 0.5714285714285714,\n",
       "    'module_dump_dir': 'modules_dumps_multiclass/prediction/threshold/comb_0'},\n",
       "   {'module_type': 'tunable',\n",
       "    'module_params': {'n_trials': None},\n",
       "    'metric_name': 'prediction_accuracy',\n",
       "    'metric_value': 0.7142857142857143,\n",
       "    'module_dump_dir': 'modules_dumps_multiclass/prediction/tunable/comb_0'},\n",
       "   {'module_type': 'tunable',\n",
       "    'module_params': {'n_trials': 3},\n",
       "    'metric_name': 'prediction_accuracy',\n",
       "    'metric_value': 0.7142857142857143,\n",
       "    'module_dump_dir': 'modules_dumps_multiclass/prediction/tunable/comb_1'},\n",
       "   {'module_type': 'argmax',\n",
       "    'module_params': {},\n",
       "    'metric_name': 'prediction_accuracy',\n",
       "    'metric_value': 0.5714285714285714,\n",
       "    'module_dump_dir': 'modules_dumps_multiclass/prediction/argmax/comb_0'},\n",
       "   {'module_type': 'jinoos',\n",
       "    'module_params': {},\n",
       "    'metric_name': 'prediction_accuracy',\n",
       "    'metric_value': 0.7142857142857143,\n",
       "    'module_dump_dir': 'modules_dumps_multiclass/prediction/jinoos/comb_0'}]}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.optimization_info.dump_evaluation_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autointent.nodes import InferenceNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== vector_db ====\n",
      "\n",
      "[2, 2, 2, 2, 1, 1, 1, 0, 1, 0] [np.float32(0.477605), np.float32(0.4929347), np.float32(0.49597514), np.float32(0.50022346), np.float32(0.553351), np.float32(0.55573064), np.float32(0.5758226), np.float32(0.60500944), np.float32(0.6174678), np.float32(0.67098904)] ['please set an alarm for mid day', 'set my alarm for getting up', 'make sure my alarm is set for three thirty in the morning', 'have an alarm set for three in the morning', 'i am nost sure why my account is blocked', 'i think my account is blocked but i do not know the reason', 'can you tell me why is my bank account frozen', 'is it possible to make a reservation at redrobin', 'why is there a hold on my american saving bank account', 'does redrobin take reservations']\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "\n",
    "for trial in context.optimization_info.trials.retrieval:\n",
    "    print(f\"\\n==== {trial.module_type} ====\\n\")\n",
    "    config = dict(\n",
    "        node_type=\"retrieval\",\n",
    "        module_type=trial.module_type,\n",
    "        module_config=trial.module_params,\n",
    "        load_path=trial.module_dump_dir,\n",
    "    )\n",
    "    node = InferenceNode(**config)\n",
    "    labels, distances, texts = node.module.predict([\"hello\", \"world\"])\n",
    "    print(labels[0], distances[0], texts[0])\n",
    "    node.module.clear_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== knn ====\n",
      "\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "==== knn ====\n",
      "\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "==== knn ====\n",
      "\n",
      "[[0.         0.         0.7611975 ]\n",
      " [0.         0.         0.74537146]]\n",
      "\n",
      "==== linear ====\n",
      "\n",
      "[[0.21546907 0.33743832 0.44709261]\n",
      " [0.22099187 0.3357609  0.44324723]]\n",
      "\n",
      "==== dnnc ====\n",
      "\n",
      "[[  0.           0.         -10.27686977]\n",
      " [  0.           0.         -10.98405933]]\n",
      "\n",
      "==== dnnc ====\n",
      "\n",
      "[[0.         0.         0.18697797]\n",
      " [0.         0.         0.07185777]]\n"
     ]
    }
   ],
   "source": [
    "for trial in context.optimization_info.trials.scoring:\n",
    "    print(f\"\\n==== {trial.module_type} ====\\n\")\n",
    "    config = dict(\n",
    "        node_type=\"scoring\",\n",
    "        module_type=trial.module_type,\n",
    "        module_config=trial.module_params,\n",
    "        load_path=trial.module_dump_dir,\n",
    "    )\n",
    "    node = InferenceNode(**config)\n",
    "    scores = node.module.predict([\"hello\", \"world\"])\n",
    "    print(scores)\n",
    "    node.module.clear_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== threshold ====\n",
      "\n",
      "[-1 -1]\n",
      "\n",
      "==== tunable ====\n",
      "\n",
      "[-1 -1]\n",
      "\n",
      "==== tunable ====\n",
      "\n",
      "[-1 -1]\n",
      "\n",
      "==== argmax ====\n",
      "\n",
      "[2 2]\n",
      "\n",
      "==== jinoos ====\n",
      "\n",
      "[-1 -1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "for trial in context.optimization_info.trials.prediction:\n",
    "    print(f\"\\n==== {trial.module_type} ====\\n\")\n",
    "    config = dict(\n",
    "        node_type=\"prediction\",\n",
    "        module_type=trial.module_type,\n",
    "        module_config=trial.module_params,\n",
    "        load_path=trial.module_dump_dir,\n",
    "    )\n",
    "    node = InferenceNode(**config)\n",
    "    scores = node.module.predict(np.array([[0.27486506, 0.31681463, 0.37459106], [0.2769358,  0.31536099, 0.37366978]]))\n",
    "    print(scores)\n",
    "    node.module.clear_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autointent-D7M6VOhJ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
